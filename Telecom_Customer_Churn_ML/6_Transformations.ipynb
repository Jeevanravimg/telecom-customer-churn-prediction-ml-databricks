{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad2f381-6e38-4b8a-ac88-f5c55fbc11fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 1 — Configuration & Imports\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "# Spark session (Databricks already provides this)\n",
    "spark = SparkSession.builder.appName(\"TelecomChurn_Transformation\").getOrCreate()\n",
    "\n",
    "# Source cleaned Delta tables\n",
    "train_table = \"kusha_solutions.telecom_churn_ml.telecom_train_clean\"\n",
    "test_table  = \"kusha_solutions.telecom_churn_ml.telecom_test_clean\"\n",
    "\n",
    "# Read data\n",
    "train_df = spark.table(train_table)\n",
    "test_df  = spark.table(test_table)\n",
    "\n",
    "print(\"✅ Cleaned Train & Test tables loaded successfully.\")\n",
    "display(train_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29107b00-f4c6-468d-af48-ab5cf649f851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec9bc83-1df6-4216-9363-9a242422d8c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 3 — Feature Scaling (Standardization)\n",
    "# ------------------------------------\n",
    "\n",
    "numeric_cols = ['tenure', 'MonthlyCharges_log', 'TotalCharges_log']\n",
    "\n",
    "# Step 1: Assemble features into vector\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol='num_features_unscaled')\n",
    "\n",
    "# Step 2: Fit scaler on train only\n",
    "scaler = StandardScaler(inputCol='num_features_unscaled', outputCol='num_features_scaled', withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(assembler.transform(train_df))\n",
    "\n",
    "# Step 3: Apply same scaler to train and test\n",
    "train_df = scaler_model.transform(assembler.transform(train_df))\n",
    "test_df  = scaler_model.transform(assembler.transform(test_df))\n",
    "\n",
    "print(\"✅ Standard scaling applied to numeric columns.\")\n",
    "display(train_df.select(\"tenure\", \"MonthlyCharges_log\", \"TotalCharges_log\", \"num_features_scaled\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0101773f-2861-4b02-be5f-33afad4be385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 4 — Encode Categorical Columns\n",
    "# ------------------------------------\n",
    "\n",
    "cat_cols = ['gender', 'Partner', 'Dependents', 'InternetService', 'Contract', 'PaymentMethod']\n",
    "\n",
    "for col_name in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "    index_model = indexer.fit(train_df)\n",
    "    train_df = index_model.transform(train_df)\n",
    "    test_df  = index_model.transform(test_df)\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_index\" for c in cat_cols],\n",
    "    outputCols=[f\"{c}_encoded\" for c in cat_cols]\n",
    ")\n",
    "encoder_model = encoder.fit(train_df)\n",
    "train_df = encoder_model.transform(train_df)\n",
    "test_df  = encoder_model.transform(test_df)\n",
    "\n",
    "print(\"✅ Categorical columns encoded successfully.\")\n",
    "display(train_df.select([f\"{c}_encoded\" for c in cat_cols]).limit(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc08bb60-3716-4520-bdf8-422d7c231f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 6 — Schema Consistency Check\n",
    "# ------------------------------------\n",
    "\n",
    "train_cols = set(train_df.columns)\n",
    "test_cols = set(test_df.columns)\n",
    "\n",
    "missing_in_test = train_cols - test_cols\n",
    "missing_in_train = test_cols - train_cols\n",
    "\n",
    "print(\"Columns missing in Test:\", missing_in_test)\n",
    "print(\"Columns missing in Train:\", missing_in_train)\n",
    "\n",
    "if not missing_in_test and not missing_in_train:\n",
    "    print(\"✅ Train & Test schema are consistent.\")\n",
    "else:\n",
    "    print(\"⚠️ Schema mismatch detected. Please align manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc8fae9-8494-4166-a9c5-f4bfca0eacb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 7 — Save Transformed Delta Tables\n",
    "# ------------------------------------\n",
    "\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kusha_solutions.telecom_churn_ml.train_transformed\")\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kusha_solutions.telecom_churn_ml.test_transformed\")\n",
    "\n",
    "print(\"✅ Transformed Train & Test tables stored successfully in Delta Lake.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
