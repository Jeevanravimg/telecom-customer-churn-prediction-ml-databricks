{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5f863c-fbd9-4b2a-972e-c1e9a73c92d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------\n",
    "# Cell 1 ‚Äî Configuration and Imports\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create Spark session (Databricks usually provides this by default)\n",
    "spark = SparkSession.builder.appName(\"DataCleaningPipeline\").getOrCreate()\n",
    "\n",
    "# Define source Delta table names (update as per your workspace)\n",
    "train_table = \"kusha_solutions.telecom_churn_ml.telecom_train\"\n",
    "test_table  = \"kusha_solutions.telecom_churn_ml.telecom_test\"\n",
    "\n",
    "print(\"‚úÖ Configuration and Imports Loaded Successfully\")\n",
    "print(f\"Train Table: {train_table}\")\n",
    "print(f\"Test Table: {test_table}\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 2 ‚Äî Utility Function: Schema + Null Check\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def show_schema_and_nulls(df, name):\n",
    "    \"\"\"\n",
    "    Displays schema, counts null or empty values per column, \n",
    "    and shows a preview of the data.\n",
    "    \"\"\"\n",
    "    print(f\"--- Schema: {name} ---\")\n",
    "    df.printSchema()\n",
    "\n",
    "    exprs = []\n",
    "    for c, dtype in df.dtypes:\n",
    "        if dtype in (\"string\",):\n",
    "            # Handle missing string values and placeholders\n",
    "            expr = F.count(\n",
    "                F.when(\n",
    "                    (F.col(c).isNull()) | \n",
    "                    (F.col(c) == \"\") | \n",
    "                    (F.col(c).isin([\"NULL\", \"None\"])), c\n",
    "                )\n",
    "            ).alias(c)\n",
    "        else:\n",
    "            # Handle missing numeric values\n",
    "            expr = F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "        exprs.append(expr)\n",
    "\n",
    "    null_counts = df.select(exprs)\n",
    "    null_counts.show(truncate=False)\n",
    "    print(f\"Row count: {df.count()}\")\n",
    "    display(df.limit(5))\n",
    "    print(\"‚úÖ Schema and Null Summary Completed\")\n",
    "\n",
    "print(\"‚úÖ Utility function defined successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 3 ‚Äî Read Train & Test Data from Delta Tables\n",
    "# ------------------------------------\n",
    "\n",
    "# Read Delta tables\n",
    "train_df = spark.table(train_table)\n",
    "test_df  = spark.table(test_table)\n",
    "\n",
    "print(\"‚úÖ Train & Test Data loaded successfully.\")\n",
    "\n",
    "# Check basic info using the utility from Cell 2\n",
    "show_schema_and_nulls(train_df, \"Train Data (Raw)\")\n",
    "show_schema_and_nulls(test_df,  \"Test Data (Raw)\")\n",
    "\n",
    "#------------------------------------\n",
    "# Cell 5 ‚Äî Data Standardization (Trim, Case, Cast)\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def standardize_formats(df):\n",
    "    \"\"\"\n",
    "    Cleans up inconsistent formatting and ensures correct data types.\n",
    "    1. Trims leading/trailing spaces.\n",
    "    2. Replaces blanks, 'NULL', 'None' with null.\n",
    "    3. Casts numeric-like columns to correct data types.\n",
    "    \"\"\"\n",
    "    # Identify string columns\n",
    "    str_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "\n",
    "    # Trim and replace empty/invalid strings with NULL\n",
    "    for c in str_cols:\n",
    "        df = df.withColumn(c, F.trim(F.col(c)))\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            F.when(\n",
    "                (F.col(c) == \"\") | \n",
    "                (F.col(c).isin([\"NULL\", \"None\"])), \n",
    "                None\n",
    "            ).otherwise(F.col(c))\n",
    "        )\n",
    "\n",
    "    # ‚úÖ Cast numeric columns correctly\n",
    "    df = df.withColumn(\"tenure\", F.col(\"tenure\").cast(\"int\"))\n",
    "    df = df.withColumn(\"SeniorCitizen\", F.col(\"SeniorCitizen\").cast(\"int\"))\n",
    "    df = df.withColumn(\"MonthlyCharges\", F.col(\"MonthlyCharges\").cast(\"double\"))\n",
    "    df = df.withColumn(\"TotalCharges\", F.col(\"TotalCharges\").cast(\"double\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply to both train and test datasets\n",
    "train_df = standardize_formats(train_df)\n",
    "test_df  = standardize_formats(test_df)\n",
    "\n",
    "print(\"‚úÖ Data Standardization Completed for Train & Test\")\n",
    "show_schema_and_nulls(train_df, \"Train ‚Äî After Standardization\")\n",
    "show_schema_and_nulls(test_df,  \"Test ‚Äî After Standardization\")\n",
    "\n",
    "\n",
    "#to read and debug\n",
    "train_df.select(\"SeniorCitizen\").distinct().show()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 4 ‚Äî Column Statistics (Descriptive Summary)\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1Ô∏è‚É£ Summary statistics for numeric columns\n",
    "numeric_cols = [c for c, t in train_df.dtypes if t in (\"int\", \"double\", \"float\", \"bigint\") and c != 'SeniorCitizen']\n",
    "print(\"üìä Numeric Column Summary:\")\n",
    "display(train_df.select(numeric_cols).summary())\n",
    "\n",
    "# 2Ô∏è‚É£ Distinct counts for categorical columns (to understand diversity of values)\n",
    "cat_cols = [c for c, t in train_df.dtypes if t == \"string\" or c=='SeniorCitizen']\n",
    "print(\"üî§ Distinct Value Counts for Categorical Columns:\")\n",
    "for c in cat_cols:\n",
    "    print(f\"{c}: {train_df.select(c).distinct().count()} unique values\")\n",
    "\n",
    "# 3Ô∏è‚É£ Optional ‚Äî show frequency of few important categorical columns\n",
    "important_cats = [\"gender\", \"Partner\", \"Dependents\", \"InternetService\", \"Contract\", \"Churn\"]\n",
    "for c in important_cats:\n",
    "    print(f\"\\n Top categories in {c}:\")\n",
    "    display(train_df.groupBy(c).count().orderBy(F.desc(\"count\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 6 ‚Äî Handling Missing Values (Null Imputation)\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1Ô∏è‚É£ Show initial missing value summary\n",
    "print(\"üîç Checking missing values before imputation:\")\n",
    "show_schema_and_nulls(train_df, \"Train Data Before Imputation\")\n",
    "\n",
    "# 2Ô∏è‚É£ Fill missing numeric columns with median\n",
    "numeric_cols = [c for c, t in train_df.dtypes if t in ('int', 'double', 'float','bigint') and c != 'SeniorCitizen']\n",
    "for col_name in numeric_cols:\n",
    "    median_val = train_df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "    train_df = train_df.fillna({col_name: median_val})\n",
    "\n",
    "# 3Ô∏è‚É£ Fill missing categorical columns with mode (most frequent value)\n",
    "categorical_cols = [c for c, t in train_df.dtypes if t == 'string' or c == 'SeniorCitizen']\n",
    "for col_name in categorical_cols:\n",
    "    mode_val = (\n",
    "        train_df.groupBy(col_name)\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .first()\n",
    "    )\n",
    "    if mode_val:\n",
    "        train_df = train_df.fillna({col_name: mode_val[0]})\n",
    "\n",
    "# 4Ô∏è‚É£ Validate after imputation\n",
    "print(\"\\n‚úÖ Missing value imputation complete. Verifying...\")\n",
    "show_schema_and_nulls(train_df, \"Train Data After Imputation\")\n",
    "\n",
    "print(\"‚úÖ Null handling completed successfully.\")\n",
    "\n",
    "\n",
    "#to read and debug\n",
    "print(\"debugging************************\")\n",
    "train_df.select(\"SeniorCitizen\").distinct().show()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 8 ‚Äî Outlier Detection and Capping (IQR Method)\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1Ô∏è‚É£ Identify numeric columns for outlier treatment\n",
    "numeric_cols = [c for c, t in train_df.dtypes if t in ('int', 'double', 'float','bigint') and c != 'SeniorCitizen']\n",
    "\n",
    "# 2Ô∏è‚É£ Compute IQR (Interquartile Range) for each numeric column\n",
    "iqr_bounds = {}\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    try:\n",
    "        q1, q3 = train_df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        iqr_bounds[col_name] = (lower_bound, upper_bound)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {col_name}: {e}\")\n",
    "\n",
    "# 3Ô∏è‚É£ Apply capping for outliers\n",
    "for col_name, (lower, upper) in iqr_bounds.items():\n",
    "    train_df = train_df.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name) < lower, lower)\n",
    "         .when(F.col(col_name) > upper, upper)\n",
    "         .otherwise(F.col(col_name))\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Outlier detection and capping completed using IQR method.\")\n",
    "\n",
    "# 4Ô∏è‚É£ Optional: Show example column statistics after capping\n",
    "for col_name in numeric_cols[:5]:  # show first 5 columns only\n",
    "    stats = train_df.select(\n",
    "        F.min(col_name).alias(\"min\"),\n",
    "        F.expr(f'percentile({col_name}, 0.5)').alias(\"median\"),\n",
    "        F.max(col_name).alias(\"max\")\n",
    "    ).first()\n",
    "    print(f\"{col_name}: min={stats['min']}, median={stats['median']}, max={stats['max']}\")\n",
    "\n",
    "\n",
    "\n",
    "#to read and debug\n",
    "print(\"debugging************************\")\n",
    "train_df.select(\"SeniorCitizen\").distinct().show()\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 9 ‚Äî Apply Cleaning (Test Data)\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1Ô∏è‚É£ Show missing values before imputation\n",
    "print(\"üîç Checking missing values in Test Data before imputation:\")\n",
    "show_schema_and_nulls(test_df, \"Test Data Before Imputation\")\n",
    "\n",
    "# 2Ô∏è‚É£ Fill missing numeric columns in test_df with median values from train_df\n",
    "numeric_cols = [c for c, t in test_df.dtypes if t in ('int', 'double', 'float') and c != 'SeniorCitizen']\n",
    "for col_name in numeric_cols:\n",
    "    median_val = train_df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "    test_df = test_df.fillna({col_name: median_val})\n",
    "\n",
    "# 3Ô∏è‚É£ Fill missing categorical columns with mode values from train_df\n",
    "categorical_cols = [c for c, t in test_df.dtypes if t == 'string' or c == 'SeniorCitizen']\n",
    "for col_name in categorical_cols:\n",
    "    mode_val = (\n",
    "        train_df.groupBy(col_name)\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .first()\n",
    "    )\n",
    "    if mode_val:\n",
    "        test_df = test_df.fillna({col_name: mode_val[0]})\n",
    "\n",
    "# 4Ô∏è‚É£ Handle outliers in numeric columns (using train_df fences)\n",
    "for col_name in numeric_cols:\n",
    "    Q1, Q3 = train_df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_fence = Q1 - 1.5 * IQR\n",
    "    upper_fence = Q3 + 1.5 * IQR\n",
    "    test_df = test_df.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name) < lower_fence, lower_fence)\n",
    "         .when(F.col(col_name) > upper_fence, upper_fence)\n",
    "         .otherwise(F.col(col_name))\n",
    "    )\n",
    "\n",
    "# 5Ô∏è‚É£ Validate final test_df\n",
    "print(\"\\n‚úÖ Test Data cleaning complete. Verifying...\")\n",
    "show_schema_and_nulls(test_df, \"Test Data After Cleaning\")\n",
    "\n",
    "print(\"‚úÖ Test dataset cleaned successfully and ready for transformation.\")\n",
    "\n",
    "\n",
    "#to read and debug\n",
    "print(\"debugging************************\")\n",
    "train_df.select(\"SeniorCitizen\").distinct().show()\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 10 ‚Äî Save Cleaned Data as Delta Tables\n",
    "# ------------------------------------\n",
    " \n",
    "clean_train_table = \"kusha_solutions.telecom_churn_ml.telecom_train_clean\"\n",
    "clean_test_table  = \"kusha_solutions.telecom_churn_ml.telecom_test_clean\"\n",
    " \n",
    "# Overwrite mode ensures updated data replaces previous versions\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(clean_train_table)\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(clean_test_table)\n",
    " \n",
    "print(f\"‚úÖ Cleaned Train Data saved as: {clean_train_table}\")\n",
    "print(f\"‚úÖ Cleaned Test Data saved as: {clean_test_table}\")\n",
    "\n",
    "\n",
    "#to read and debug\n",
    "print(\"debugging************************\")\n",
    "train_df.select(\"SeniorCitizen\").distinct().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b3cf770-5e92-438f-8a97-4fd770856b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b86b61-d7dd-49ec-800b-327921032526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1Ô∏è‚É£ Summary statistics for numeric columns\n",
    "numeric_cols = [c for c, t in train_df.dtypes if t in (\"int\", \"double\", \"float\", \"bigint\")]\n",
    "print(\"üìä Numeric Column Summary:\")\n",
    "display(train_df.select(numeric_cols).summary())\n",
    "\n",
    "# 2Ô∏è‚É£ Distinct counts for categorical columns (to understand diversity of values)\n",
    "cat_cols = [c for c, t in train_df.dtypes if t == \"string\"]\n",
    "print(\"üî§ Distinct Value Counts for Categorical Columns:\")\n",
    "for c in cat_cols:\n",
    "    print(f\"{c}: {train_df.select(c).distinct().count()} unique values\")\n",
    "\n",
    "# 3Ô∏è‚É£ Optional ‚Äî show frequency of few important categorical columns\n",
    "important_cats = [\"gender\", \"Partner\", \"Dependents\", \"InternetService\", \"Contract\", \"Churn\"]\n",
    "for c in important_cats:\n",
    "    print(f\"\\n Top categories in {c}:\")\n",
    "    display(train_df.groupBy(c).count().orderBy(F.desc(\"count\")))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Data_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
