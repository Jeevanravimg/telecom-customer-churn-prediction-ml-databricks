{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a9c2ea-5444-48fa-a99c-7b3e0556152f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 1 ‚Äî Configuration and Imports\n",
    "# ------------------------------------\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, when, count, isnan\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Load the balanced dataset (after SMOTE)\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "train_df = spark.table(\"kusha_solutions.telecom_churn_ml.train_balanced\")\n",
    "\n",
    "print(\"‚úÖ Balanced data loaded successfully for Feature Engineering\")\n",
    "print(\"Row count:\", train_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efa1393-7702-4a9b-9660-2aff7a1f46d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 2 ‚Äî Create Derived Features\n",
    "# ------------------------------------\n",
    "\n",
    "# 1Ô∏è‚É£ Average Monthly Spend (TotalCharges / tenure)\n",
    "train_df = train_df.withColumn(\n",
    "    \"AvgMonthlySpend\",\n",
    "    F.when(col(\"tenure\") > 0, col(\"TotalCharges\") / col(\"tenure\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Payment Behavior Ratio\n",
    "train_df = train_df.withColumn(\n",
    "    \"Monthly_to_Total_Ratio\",\n",
    "    F.when(col(\"TotalCharges\") > 0, col(\"MonthlyCharges\") / col(\"TotalCharges\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Has Internet connection\n",
    "train_df = train_df.withColumn(\"HasInternet\", when(col(\"InternetService_index\") != 0, 1).otherwise(0))\n",
    "\n",
    "# 5Ô∏è‚É£ Number of active services (counts of 'Yes' in service features)\n",
    "service_features = [\"PhoneService\", \"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "                    \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"ActiveServiceCount\",\n",
    "    reduce(lambda a, b: a + b, [when(col(c) == 1, 1).otherwise(0) for c in service_features])\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created new domain-based features successfully.\")\n",
    "display(train_df.select(\"AvgMonthlySpend\", \"Monthly_to_Total_Ratio\", \"HasInternet\", \"ActiveServiceCount\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5af85f7-0bc3-4147-ac3d-cb50a8c3c1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 3 ‚Äî Feature Correlation Analysis\n",
    "# ------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "train_pd = train_df.select(\n",
    "    \"tenure\", \"MonthlyCharges\", \"TotalCharges\", \"AvgMonthlySpend\",\n",
    "    \"Monthly_to_Total_Ratio\", \"ActiveServiceCount\", \"HasInternet\", \"Churn_index\"\n",
    ").toPandas()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = train_pd.corr()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a216c249-c760-45e5-9248-9ee0126294d6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1762945823257}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 4 ‚Äî Drop Redundant or Unnecessary Columns\n",
    "# ------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Columns that don't add predictive value or are redundant\n",
    "cols_to_drop = [\n",
    "    \"customerID\",                # Unique identifier\n",
    "    \"TotalCharges\",              # Replaced by TotalCharges_log\n",
    "    \"MonthlyCharges\",            # Replaced by MonthlyCharges_log\n",
    "    \"num_features_scaled\",       # Temporary technical column\n",
    "    \"num_features_unscaled\"      # Temporary technical column\n",
    "]\n",
    "\n",
    "# Drop only if columns exist in the DataFrame\n",
    "train_df = train_df.drop(*[c for c in cols_to_drop if c in train_df.columns])\n",
    "\n",
    "print(\"‚úÖ Dropped redundant/unnecessary columns successfully.\")\n",
    "print(\"Remaining columns count:\", len(train_df.columns))\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e549891-45f2-45bd-b6a3-3613bd142a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "feature_cols = [c for c, t in train_df.dtypes if t in (\"int\", \"double\", \"float\",\"long\") and c != \"Churn_index\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_vec = assembler.transform(train_df)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Churn_index\", numTrees=30, maxDepth=5)\n",
    "model = rf.fit(train_vec)\n",
    "\n",
    "importances = model.featureImportances\n",
    "feature_importance = list(zip(feature_cols, importances))\n",
    "importance_df = pd.DataFrame(feature_importance, columns=[\"Feature\", \"Importance\"])\n",
    "importance_df[\"Rank\"] = importance_df[\"Importance\"].rank(method=\"first\", ascending=False)\n",
    "importance_df = importance_df.sort_values(\"Rank\").reset_index(drop=True)\n",
    "\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3f7149-ecf2-4722-9899-c5e47dc5a229",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762842915517}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_b0f6286f\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_bd7ecf74\",\"enabled\":true,\"columnId\":\"Churn_index\",\"dataType\":\"float\",\"filterType\":\"oneof\",\"filterValues\":[2],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1762942793184}],\"syncTimestamp\":1762942793184}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"Churn_index\"},{\"kind\":\"literal\",\"value\":2,\"type\":\"float\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 6 ‚Äî Drop Low-Importance Features Automatically\n",
    "# ------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert feature importance (from Random Forest) into a Pandas DataFrame if not already\n",
    "# importance_df = pd.DataFrame(feature_importance, columns=[\"Feature\", \"Importance\"])\n",
    "\n",
    "# ‚úÖ Step 1: Define threshold (features with importance < 0.01 will be dropped)\n",
    "importance_threshold = 0.01\n",
    "\n",
    "# ‚úÖ Step 2: Identify features to drop\n",
    "low_importance_features = importance_df[importance_df[\"Importance\"] < importance_threshold][\"Feature\"].tolist()\n",
    "\n",
    "print(\"üîª Features to Drop (Low Importance):\")\n",
    "for f in low_importance_features:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "# ‚úÖ Step 3: Drop those columns from train_df\n",
    "train_df = train_df.drop(*[c for c in low_importance_features if c in train_df.columns])\n",
    "\n",
    "print(f\"\\n‚úÖ Dropped {len(low_importance_features)} low-importance features successfully.\")\n",
    "print(\"Remaining columns:\", len(train_df.columns))\n",
    "display(train_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37bec2af-a12b-41de-9905-c970815e68dc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"AvgMonthlySpend\":174},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762844585663}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 7 ‚Äî Feature Reordering Based on Importance\n",
    "# ------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ‚úÖ Step 1: Extract the remaining high-importance features (already filtered in previous cell)\n",
    "ordered_features = (\n",
    "    importance_df[importance_df[\"Importance\"] >= 0.01]\n",
    "    .sort_values(\"Importance\", ascending=False)[\"Feature\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# ‚úÖ Step 2: Append target column 'Churn_index' at the end\n",
    "ordered_features.append(\"Churn_index\")\n",
    "\n",
    "# ‚úÖ Step 3: Reorder columns in train_df (keep only the ordered features)\n",
    "train_df = train_df.select([c for c in ordered_features if c in train_df.columns])\n",
    "\n",
    "# ‚úÖ Step 4: Verify new column order\n",
    "print(\"‚úÖ Features reordered successfully based on importance ranking.\")\n",
    "print(\"New column order (most important first):\")\n",
    "print(train_df.columns)\n",
    "\n",
    "# ‚úÖ Step 5: Display a few records to confirm\n",
    "display(train_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f795c9-7f21-4790-aada-a9aaf732c59c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762942824408}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_ac8d15\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_f6ed5acc\",\"enabled\":true,\"columnId\":\"Churn_index\",\"dataType\":\"float\",\"filterType\":\"oneof\",\"filterValues\":[2],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1762942812412},{\"enabled\":true,\"filterGroupId\":\"fg_c37f1543\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_ec328047\",\"enabled\":true,\"columnId\":\"Churn_index\",\"dataType\":\"float\",\"filterType\":\"oneof\",\"filterValues\":[2],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1762942947879}],\"syncTimestamp\":1762942947879}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"and\",\"args\":[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"Churn_index\"},{\"kind\":\"literal\",\"value\":2,\"type\":\"float\"}]}]},{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"Churn_index\"},{\"kind\":\"literal\",\"value\":2,\"type\":\"float\"}]}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell ‚Äî VectorAssembler\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# ‚úÖ Step 1: Define feature columns (exclude target)\n",
    "feature_cols = [c for c in train_df.columns if c != \"Churn_index\"]\n",
    "\n",
    "# ‚úÖ Step 2: Assemble all features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Step 3: Transform the dataset to create 'features' column\n",
    "train_vec = assembler.transform(train_df)\n",
    "\n",
    "print(\"‚úÖ VectorAssembler completed successfully.\")\n",
    "print(f\"Total features combined: {len(feature_cols)}\")\n",
    "\n",
    "# ‚úÖ Optional: Preview\n",
    "display(train_vec.select(\"features\", \"Churn_index\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede26883-b377-43b0-b7b2-88d5fb614196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell ‚Äî StandardScaler (Feature Scaling)\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# ‚úÖ Step 1: Initialize StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",          # the vector created from VectorAssembler\n",
    "    outputCol=\"features_scaled\",  # new column for scaled features\n",
    "    withMean=True,                # center the data (mean = 0)\n",
    "    withStd=True                  # scale to unit variance (std = 1)\n",
    ")\n",
    "\n",
    "# ‚úÖ Step 2: Fit the scaler on training data\n",
    "scaler_model = scaler.fit(train_vec)\n",
    "\n",
    "# ‚úÖ Step 3: Transform data to create scaled feature vector\n",
    "train_scaled = scaler_model.transform(train_vec)\n",
    "\n",
    "print(\"‚úÖ StandardScaler applied successfully.\")\n",
    "display(train_scaled.select(\"features_scaled\", \"Churn_index\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb21396-5e50-4a2c-bdd0-c118f40da1ce",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_1d075e2b\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_eb3268ba\",\"enabled\":true,\"columnId\":\"Churn_index\",\"dataType\":\"float\",\"filterType\":\"oneof\",\"filterValues\":[2],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1762942972990}],\"syncTimestamp\":1762942972991}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"Churn_index\"},{\"kind\":\"literal\",\"value\":2,\"type\":\"float\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell ‚Äî Store Transformed & Scaled Dataset\n",
    "# ------------------------------------\n",
    "\n",
    "# ‚úÖ Define target Delta table name\n",
    "transformed_table = \"kusha_solutions.telecom_churn_ml.train_final_featured_transformed\"\n",
    "\n",
    "# ‚úÖ Select only required columns for model training\n",
    "# Keeping: scaled features + target column\n",
    "final_train_df = train_scaled.select(\"features_scaled\", \"Churn_index\")\n",
    "\n",
    "# ‚úÖ Store the final transformed dataset into Delta table\n",
    "final_train_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(transformed_table)\n",
    "\n",
    "print(f\"‚úÖ Final transformed and scaled training data stored successfully as: {transformed_table}\")\n",
    "display(spark.table(transformed_table).limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2945826-f25e-4ffa-a41a-3f03434d5244",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762946257170}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Cell 1 ‚Äî Load Test Dataset\n",
    "# ------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "test_df = spark.table(\"kusha_solutions.telecom_churn_ml.test_transformed\")\n",
    "print(\"‚úÖ Test data loaded successfully\")\n",
    "print(\"Row count:\", test_df.count())\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 2 ‚Äî Create Derived Features (Same as Train)\n",
    "# ------------------------------------\n",
    "test_df = test_df.withColumn(\n",
    "    \"AvgMonthlySpend\",\n",
    "    F.when(col(\"tenure\") > 0, col(\"TotalCharges\") / col(\"tenure\")).otherwise(0)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"Monthly_to_Total_Ratio\",\n",
    "    F.when(col(\"TotalCharges\") > 0, col(\"MonthlyCharges\") / col(\"TotalCharges\")).otherwise(0)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"HasInternet\",\n",
    "    when(col(\"InternetService_index\") != 0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# ------------------------------------\n",
    "# Fix: Ensure categorical service features are numeric before ActiveServiceCount\n",
    "# ------------------------------------\n",
    "service_features = [\n",
    "    \"PhoneService\", \"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "    \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"\n",
    "]\n",
    "\n",
    "for c in service_features:\n",
    "    if c in test_df.columns:\n",
    "        test_df = test_df.withColumn(\n",
    "            c,\n",
    "            when(col(c) == 'Yes', 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "# Now safely create ActiveServiceCount\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"ActiveServiceCount\",\n",
    "    reduce(lambda a, b: a + b, [when(col(c) == 1, 1).otherwise(0) for c in service_features])\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Derived features added to test data\")\n",
    "display(test_df.select(\"AvgMonthlySpend\", \"Monthly_to_Total_Ratio\", \"HasInternet\", \"ActiveServiceCount\").limit(5))\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 3 ‚Äî Encode Target Column (Churn ‚Üí Churn_index)\n",
    "# ------------------------------------\n",
    "# Encode Churn as numeric labels for ML (0,1,2)\n",
    "test_df = test_df.withColumn(\n",
    "    \"Churn_index\",\n",
    "    when(col(\"Churn\") == \"No\", 0)\n",
    "    .when(col(\"Churn\") == \"Yes\", 2)\n",
    "    .otherwise(1)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Churn column encoded as Churn_index\")\n",
    "display(test_df.select(\"Churn\", \"Churn_index\").distinct())\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 4 ‚Äî Drop Redundant/Low-Importance Features\n",
    "# ------------------------------------\n",
    "cols_to_drop_train = [\n",
    "    \"customerID\", \"TotalCharges\", \"MonthlyCharges\",\n",
    "    \"num_features_scaled\", \"num_features_unscaled\", \"Churn\"  # drop old churn text column\n",
    "]\n",
    "\n",
    "all_cols_to_drop = [c for c in cols_to_drop_train + low_importance_features if c in test_df.columns]\n",
    "test_df = test_df.drop(*all_cols_to_drop)\n",
    "\n",
    "print(f\"‚úÖ Dropped redundant and low-importance features: {len(all_cols_to_drop)} columns\")\n",
    "display(test_df.limit(5))\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 5 ‚Äî Reorder Features (Same Order as Train)\n",
    "# ------------------------------------\n",
    "ordered_features_no_target = [c for c in ordered_features if c != \"Churn_index\" and c in test_df.columns]\n",
    "test_df = test_df.select(ordered_features_no_target + [\"Churn_index\"])\n",
    "\n",
    "print(\"‚úÖ Test features reordered to match training order\")\n",
    "display(test_df.limit(5))\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 6 ‚Äî Apply VectorAssembler and StandardScaler\n",
    "# ------------------------------------\n",
    "test_vec = assembler.transform(test_df)\n",
    "print(\"‚úÖ VectorAssembler applied to test data\")\n",
    "\n",
    "test_scaled = scaler_model.transform(test_vec)\n",
    "print(\"‚úÖ StandardScaler applied (no data leakage)\")\n",
    "display(test_scaled.select(\"features_scaled\", \"Churn_index\").limit(5))\n",
    "\n",
    "# ------------------------------------\n",
    "# Cell 7 ‚Äî Save Transformed Test Dataset\n",
    "# ------------------------------------\n",
    "final_test_table = \"kusha_solutions.telecom_churn_ml.test_final_featured_transformed\"\n",
    "\n",
    "final_test_df = test_scaled.select(\"features_scaled\", \"Churn_index\")\n",
    "\n",
    "final_test_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(final_test_table)\n",
    "\n",
    "print(f\"‚úÖ Test data transformed and saved successfully as: {final_test_table}\")\n",
    "display(spark.table(final_test_table).limit(5))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5714963804110410,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "8_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
