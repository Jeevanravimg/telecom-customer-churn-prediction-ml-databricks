# Telecom Customer Churn Prediction â€“ End-to-End ML Pipeline (Databricks)

## ğŸ“Œ Project Overview
This project implements a **production-grade, end-to-end Machine Learning pipeline** to predict **customer churn** in the telecom domain.  
The solution is built using **Databricks, PySpark, Delta Lake, and MLflow**, following **modern data engineering and MLOps best practices**.

The pipeline covers the **entire ML lifecycle**:
- Data generation
- Ingestion into Delta Lake (Bronze â†’ Silver â†’ Gold)
- Cleaning & validation
- Exploratory Data Analysis (EDA)
- Feature engineering
- Handling class imbalance (SMOTE)
- Model training & evaluation
- Model registration and inference readiness

---

## ğŸ—ï¸ Architecture & Design Principles

- **Medallion Architecture**
  - **Bronze**: Raw ingested data
  - **Silver**: Cleaned, validated, standardized data
  - **Gold**: Feature-engineered, model-ready & inference outputs
- **No data leakage** (train-test split before cleaning & SMOTE only on train)
- **Reproducibility** using MLflow
- **Scalable** using Spark & Delta Lake
- **Modular pipeline** using Databricks notebooks

---

## ğŸ§° Tech Stack

- **Databricks**
- **Apache Spark (PySpark)**
- **Delta Lake**
- **Python**
- **Scikit-learn**
- **MLflow**
- **Pandas, NumPy**
- **Matplotlib, Seaborn**
- **Faker (Synthetic data generation)**

---

## ğŸ“‚ Project Structure

telecom-customer-churn-ml/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 0_Pipeline.ipynb
â”‚ â”œâ”€â”€ 1_Data_Gen_Col.ipynb
â”‚ â”œâ”€â”€ 2_Data_Ingestion.ipynb
â”‚ â”œâ”€â”€ 3_Data_Splitting.ipynb
â”‚ â”œâ”€â”€ 4_Data_Cleaning.ipynb
â”‚ â”œâ”€â”€ 5_EDA.ipynb
â”‚ â”œâ”€â”€ 6_Transformations.ipynb
â”‚ â”œâ”€â”€ 7_SMOTE.ipynb
â”‚ â”œâ”€â”€ 8_Feature_Engineering.ipynb
â”‚ â””â”€â”€ 9_ML_Pipeline_Flow.ipynb
â”‚
â”‚
â”œâ”€â”€ README.md


---

## ğŸ”„ End-to-End Pipeline Description

### 1ï¸âƒ£ Data Generation & Collection
- Loaded original Telco Customer Churn dataset.
- Generated **5,00,000 synthetic records** using **Faker** to simulate large-scale production data.
- Synthetic churn logic based on:
  - Contract type
  - Tenure
  - Monthly charges
- Combined original and generated data.

---

### 2ï¸âƒ£ Data Ingestion (Bronze Layer)
- Ingested raw data into **Delta tables**.
- Ensures traceability and reproducibility.

---

### 3ï¸âƒ£ Train-Test Split (Before Cleaning)
- Converted Spark â†’ Pandas for stratified split.
- **80% Train / 20% Test**
- Stratified on target (`Churn`) to preserve class distribution.

âœ”ï¸ Prevents data leakage and simulates real-world ingestion.

---

### 4ï¸âƒ£ Data Cleaning & Validation (Silver Layer)

#### âœ” Schema Validation
- Verified column types
- Checked nulls, blanks, invalid values

#### âœ” Standardization
- Trimmed strings
- Replaced `NULL`, `None`, empty strings
- Corrected numeric datatypes

#### âœ” Missing Value Handling
- Numeric â†’ Median
- Categorical â†’ Mode (from training data)

#### âœ” Outlier Treatment
- IQR-based capping
- Applied **train fences to test data**


---

### 5ï¸âƒ£ Exploratory Data Analysis (EDA)
- Target distribution analysis
- Numeric feature distributions
- Boxplots for outliers
- Categorical frequency analysis
- Feature vs Churn analysis
- Correlation heatmaps

ğŸ“Š Ensured no target leakage during EDA.

---

### 6ï¸âƒ£ Data Transformation
- Log transformation for skewed numeric features
- Standard scaling using `StandardScaler`
- Categorical encoding:
  - `StringIndexer`
  - `OneHotEncoder`
- Schema consistency checks between train & test


---

### 7ï¸âƒ£ Handling Class Imbalance (SMOTE)
- Converted Spark â†’ Pandas
- Applied **SMOTE only on training data**
- Ensured all features were numeric
- Re-converted balanced dataset to Spark

âœ”ï¸ Prevented synthetic data leakage into test set.

---

### 8ï¸âƒ£ Feature Engineering
- Created domain-specific features:
  - `AvgMonthlySpend`
  - `Monthly_to_Total_Ratio`
  - `HasInternet`
  - `ActiveServiceCount`
- Feature correlation analysis
- Feature importance using Random Forest
- Dropped low-importance features
- Reordered features by importance

---

### 9ï¸âƒ£ Feature Assembly & Scaling
- Combined features using `VectorAssembler`
- Applied `StandardScaler` on vectorized features

---

### ğŸ”Ÿ Model Training (Pure Python â€“ Sklearn)
- Converted Spark vectors â†’ Pandas arrays
- Trained **optimized RandomForestClassifier**
- Hyperparameters tuned for speed & generalization
- Evaluated on **exact same Spark test set**

#### ğŸ“Š Metrics
- Accuracy
- Precision
- Recall
- F1-Score
- Confusion Matrix

---

### 1ï¸âƒ£1ï¸âƒ£ Experiment Tracking & Model Registry
- Logged:
  - Parameters
  - Metrics
  - Model artifacts
- Registered model:


âœ”ï¸ Versioned, auditable, production-ready.

---

### 1ï¸âƒ£2ï¸âƒ£ Model Loading & Inference
- Loaded registered model using MLflow
- Performed sample predictions
- Ready for:
  - REST API serving
  - Batch scoring
  - Databricks Model Serving

---

## ğŸ“ˆ Business Impact
- Identifies customers likely to churn
- Enables proactive retention strategies
- Scales to large telecom datasets
- Production-ready architecture

---

## ğŸš€ Future Enhancements
- Real-time inference via REST API
- Data drift monitoring
- Automated retraining pipeline
- CI/CD with Databricks Asset Bundles
- Integration with dashboards (Power BI / Tableau)

---

## ğŸ‘¤ Author
**Jeevan M G**  
Machine Learning & Data Engineering Enthusiast  
Built using Databricks, Spark, and MLflow





